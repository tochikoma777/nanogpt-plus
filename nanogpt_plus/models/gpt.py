"""
NanoGPT-Plus æ ¸å¿ƒæ¨¡å‹å®ç°
å®Œæ•´çš„GPTï¼ˆGenerative Pre-trained Transformerï¼‰æ¶æ„
"""

import math
from typing import Optional, Tuple

import torch
import torch.nn as nn
from torch.nn import functional as F

from nanogpt_plus.config import ModelConfig


class LayerNorm(nn.Module):
    """
    å±‚å½’ä¸€åŒ–ï¼ˆLayer Normalizationï¼‰
    
    ä½œç”¨ï¼šç¨³å®šæ·±å±‚ç½‘ç»œè®­ç»ƒï¼Œè®©æ¯ä¸€å±‚çš„è¾“å‡ºåˆ†å¸ƒä¿æŒä¸€è‡´
    
    ä¸æ™®é€šLayerNormçš„åŒºåˆ«ï¼šå¯ä»¥æ§åˆ¶æ˜¯å¦ä½¿ç”¨biasï¼ˆGPT-2ä¸ç”¨biasï¼‰
    """
    
    def __init__(self, ndim: int, bias: bool = True):
        """
        å‚æ•°:
            ndim: è¾“å…¥ç‰¹å¾çš„ç»´åº¦ï¼ˆå¦‚768ï¼‰
            bias: æ˜¯å¦ä½¿ç”¨åç½®é¡¹ï¼ˆGPT-2è®¾ä¸ºFalseï¼‰
        """
        super().__init__()
        # å¯å­¦ä¹ çš„ç¼©æ”¾å‚æ•°ï¼ˆgammaï¼‰ï¼Œåˆå§‹ä¸º1ï¼ˆä¸æ”¹å˜ï¼‰
        self.weight = nn.Parameter(torch.ones(ndim))
        # å¯å­¦ä¹ çš„åç§»å‚æ•°ï¼ˆbetaï¼‰ï¼Œåˆå§‹ä¸º0ï¼ˆä¸æ”¹å˜ï¼‰
        # å¦‚æœbias=Falseï¼Œåˆ™ä¸åˆ›å»ºï¼ˆèŠ‚çœå‚æ•°ï¼‰
        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None
    
    def forward(self, input: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­ï¼šå¯¹æœ€åä¸€ä¸ªç»´åº¦åšå½’ä¸€åŒ–
        
        è¾“å…¥å½¢çŠ¶: (batch_size, seq_len, ndim)
        è¾“å‡ºå½¢çŠ¶: ç›¸åŒ
        """
        # F.layer_normæ˜¯PyTorchä¼˜åŒ–è¿‡çš„å®ç°ï¼Œæ¯”æ‰‹åŠ¨è®¡ç®—æ›´å¿«
        return F.layer_norm(
            input, 
            self.weight.shape,  # å½’ä¸€åŒ–çš„ç»´åº¦
            self.weight,        # ç¼©æ”¾
            self.bias,          # åç§»ï¼ˆå¯èƒ½ä¸ºNoneï¼‰
            1e-5                # é˜²æ­¢é™¤ä»¥0çš„å°æ•°
        )


class CausalSelfAttention(nn.Module):
    """
    å› æœè‡ªæ³¨æ„åŠ›ï¼ˆCausal Self-Attentionï¼‰
    
    "å› æœ" = åªèƒ½çœ‹å½“å‰å’Œä¹‹å‰çš„tokenï¼Œä¸èƒ½å·çœ‹æœªæ¥ï¼ˆè‡ªå›å½’ç‰¹æ€§ï¼‰
    "è‡ª" = Query/Key/Valueéƒ½æ¥è‡ªåŒä¸€ä¸ªè¾“å…¥
    """
    
    def __init__(self, config: ModelConfig):
        super().__init__()
        # æ–­è¨€ï¼šç¡®ä¿ç»´åº¦èƒ½è¢«å¤´æ•°æ•´é™¤
        assert config.n_embd % config.n_head == 0, "n_embdå¿…é¡»èƒ½è¢«n_headæ•´é™¤"
        
        # ä¿å­˜é…ç½®
        self.n_head = config.n_head      # æ³¨æ„åŠ›å¤´æ•°ï¼ˆå¦‚12ï¼‰
        self.n_embd = config.n_embd      # æ€»ç»´åº¦ï¼ˆå¦‚768ï¼‰
        self.head_dim = config.head_dim  # æ¯å¤´ç»´åº¦ï¼ˆ768/12=64ï¼‰
        self.dropout = config.dropout
        
        # å…³é”®ä¼˜åŒ–ï¼šåˆå¹¶QKVæŠ•å½±ä¸ºä¸€ä¸ªå¤§çŸ©é˜µ
        # è€Œä¸æ˜¯ä¸‰ä¸ªç‹¬ç«‹å±‚ï¼Œå‡å°‘kernelå¯åŠ¨å¼€é”€
        # å½¢çŠ¶: (n_embd, 3*n_embd) = (768, 2304)
        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)
        
        # è¾“å‡ºæŠ•å½±ï¼šæŠŠæ³¨æ„åŠ›ç»“æœæ˜ å°„å›n_embd
        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)
        
        # Dropoutæ­£åˆ™åŒ–
        self.attn_dropout = nn.Dropout(config.dropout)
        self.resid_dropout = nn.Dropout(config.dropout)
        
        # å› æœæ©ç ï¼šä¸Šä¸‰è§’ä¸º0ï¼ˆç¦æ­¢çœ‹æœªæ¥ï¼‰
        # æ³¨å†Œä¸ºbufferï¼ˆä¸æ˜¯å‚æ•°ï¼Œä¸è®­ç»ƒï¼Œä½†éšæ¨¡å‹ä¿å­˜ï¼‰
        self.register_buffer(
            "bias",
            torch.tril(torch.ones(config.block_size, config.block_size))
            .view(1, 1, config.block_size, config.block_size)
        )
        # ç»“æœå½¢çŠ¶: (1, 1, block_size, block_size)
        # å‰ä¸¤ä¸ª1æ˜¯ä¸ºäº†å¹¿æ’­åˆ°(batch, n_head)
        
        # Flash Attentionæ£€æµ‹ï¼ˆPyTorch 2.0+å†…ç½®ï¼‰
        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')
        if not self.flash:
            print("âš ï¸ è­¦å‘Š: Flash Attentionä¸å¯ç”¨ï¼Œä½¿ç”¨æ ‡å‡†å®ç°ï¼ˆè¾ƒæ…¢ï¼‰")
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        è¾“å…¥x: (batch_size, seq_len, n_embd)
        è¾“å‡º: ç›¸åŒå½¢çŠ¶
        """
        B, T, C = x.size()  # Batch, Time (seq_len), Channels (n_embd)
        
        # æ­¥éª¤1: è®¡ç®—QKVï¼ˆåˆå¹¶æŠ•å½±ååˆ†å‰²ï¼‰
        # qkv: (B, T, 3*C) -> åˆ†å‰²ä¸º3ä¸ª(B, T, C)
        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)
        
        # æ­¥éª¤2: é‡å¡‘ä¸ºå¤šå¤´æ ¼å¼
        # ä» (B, T, C) -> (B, T, n_head, head_dim) -> (B, n_head, T, head_dim)
        # è¿™æ ·æ¯ä¸ªå¤´å¯ä»¥ç‹¬ç«‹è®¡ç®—æ³¨æ„åŠ›
        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)
        k = k.view(B, T, self.n_head, self.head_dim).transpose(1, 2)
        v = v.view(B, T, self.n_head, self.head_dim).transpose(1, 2)
        # ç°åœ¨å½¢çŠ¶: (B, n_head, T, head_dim)
        
        # æ­¥éª¤3: æ³¨æ„åŠ›è®¡ç®—
        if self.flash:
            # âœ… Flash Attention: èåˆkernelï¼Œæ›´å¿«æ›´çœæ˜¾å­˜
            y = torch.nn.functional.scaled_dot_product_attention(
                q, k, v,
                attn_mask=None,           # ä¸éœ€è¦æ˜¾å¼mask
                dropout_p=self.dropout if self.training else 0,
                is_causal=True            # è‡ªåŠ¨å¤„ç†å› æœæ©ç 
            )
        else:
            # æ‰‹åŠ¨å®ç°ï¼ˆå…¼å®¹æ—§PyTorchï¼‰
            # æ³¨æ„åŠ›åˆ†æ•°: (B, n_head, T, head_dim) @ (B, n_head, head_dim, T) 
            #         -> (B, n_head, T, T)
            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
            
            # åº”ç”¨å› æœæ©ç ï¼šæŠŠæœªæ¥ä½ç½®è®¾ä¸º-infï¼ˆsoftmaxåå˜ä¸º0ï¼‰
            # self.bias[:, :, :T, :T] å–å‡ºå½“å‰é•¿åº¦çš„å­çŸ©é˜µ
            # == 0çš„ä½ç½®è¡¨ç¤º"ç¦æ­¢çœ‹"
            att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))
            
            # Softmaxå½’ä¸€åŒ–ï¼ˆæ¯è¡Œä¹‹å’Œä¸º1ï¼‰
            att = F.softmax(att, dim=-1)
            att = self.attn_dropout(att)
            
            # åŠ æƒæ±‚å’Œ: (B, n_head, T, T) @ (B, n_head, T, head_dim)
            #        -> (B, n_head, T, head_dim)
            y = att @ v
        
        # æ­¥éª¤4: åˆå¹¶å¤šå¤´ç»“æœ
        # ä» (B, n_head, T, head_dim) è½¬å› (B, T, n_head, head_dim)
        y = y.transpose(1, 2)
        # è¿ç»­åŒ–å†…å­˜å¸ƒå±€ï¼ˆviewéœ€è¦è¿ç»­ï¼‰ï¼Œç„¶ååˆå¹¶ä¸º(B, T, C)
        y = y.contiguous().view(B, T, C)
        
        # æ­¥éª¤5: è¾“å‡ºæŠ•å½± + Dropout
        y = self.resid_dropout(self.c_proj(y))
        return y


class MLP(nn.Module):
    """
    å¤šå±‚æ„ŸçŸ¥æœºï¼ˆå‰é¦ˆç½‘ç»œï¼‰
    
    GPT-2ä½¿ç”¨GELUæ¿€æ´»ï¼Œæ‰©å±•4å€ç»´åº¦å†å‹ç¼©å›æ¥
    ç»“æ„: Linear -> GELU -> Linear
    """
    
    def __init__(self, config: ModelConfig):
        super().__init__()
        # ç¬¬ä¸€å±‚: æ‰©å±•4å€ï¼ˆ768 -> 3072ï¼‰
        # æ›´å¤šå‚æ•° = æ›´å¼ºçš„éçº¿æ€§è¡¨è¾¾èƒ½åŠ›
        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)
        
        # GELUæ¿€æ´»: å¹³æ»‘çš„ReLUå˜ä½“ï¼Œè´Ÿæ•°åŒºåŸŸä¹Ÿæœ‰å¾®å°æ¢¯åº¦
        self.gelu = nn.GELU()
        
        # ç¬¬äºŒå±‚: å‹ç¼©å›åŸå§‹ç»´åº¦ï¼ˆ3072 -> 768ï¼‰
        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)
        
        self.dropout = nn.Dropout(config.dropout)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.c_fc(x)   # æ‰©å±•
        x = self.gelu(x)   # éçº¿æ€§å˜æ¢
        x = self.c_proj(x) # å‹ç¼©
        x = self.dropout(x)
        return x


class Block(nn.Module):
    """
    Transformerå—ï¼ˆæ ¸å¿ƒæ„å»ºå•å…ƒï¼‰
    
    ç»“æ„ï¼ˆPre-LNï¼‰:
        x = x + Attention(LN(x))   # æ®‹å·®è¿æ¥1
        x = x + MLP(LN(x))          # æ®‹å·®è¿æ¥2
    
    ä¸ºä»€ä¹ˆå«"Pre-LN"ï¼ŸLayerNormæ”¾åœ¨å­å±‚ä¹‹å‰ï¼ˆè€Œéä¹‹åï¼‰
    å¥½å¤„: è®­ç»ƒæ›´ç¨³å®šï¼Œæ¢¯åº¦æµåŠ¨æ›´é¡ºç•…
    """
    
    def __init__(self, config: ModelConfig):
        super().__init__()
        # ç¬¬ä¸€ä¸ªLayerNormï¼ˆAttentionä¹‹å‰ï¼‰
        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)
        # å¤šå¤´æ³¨æ„åŠ›
        self.attn = CausalSelfAttention(config)
        # ç¬¬äºŒä¸ªLayerNormï¼ˆMLPä¹‹å‰ï¼‰
        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)
        # å‰é¦ˆç½‘ç»œ
        self.mlp = MLP(config)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # æ®‹å·®è¿æ¥1: å…ˆå½’ä¸€åŒ–ï¼Œå†Attentionï¼Œå†åŠ å›è¾“å…¥
        x = x + self.attn(self.ln_1(x))
        # æ®‹å·®è¿æ¥2: å…ˆå½’ä¸€åŒ–ï¼Œå†MLPï¼Œå†åŠ å›è¾“å…¥
        x = x + self.mlp(self.ln_2(x))
        return x


class GPT(nn.Module):
    """
    å®Œæ•´çš„GPTæ¨¡å‹
    
    ç»„æˆ:
    1. è¯åµŒå…¥ï¼ˆwteï¼‰: token -> å‘é‡
    2. ä½ç½®åµŒå…¥ï¼ˆwpeï¼‰: ä½ç½® -> å‘é‡  
    3. Dropout
    4. Nä¸ªTransformerå—ï¼ˆå †å ï¼‰
    5. æœ€ç»ˆLayerNorm
    6. è¯­è¨€æ¨¡å‹å¤´ï¼ˆlm_headï¼‰: å‘é‡ -> tokenæ¦‚ç‡
    
    å…³é”®æŠ€å·§: æƒé‡ç»‘å®šï¼ˆweight tyingï¼‰
    wteï¼ˆè¾“å…¥åµŒå…¥ï¼‰å’Œlm_headï¼ˆè¾“å‡ºæŠ•å½±ï¼‰å…±äº«æƒé‡çŸ©é˜µ
    å‡å°‘å‚æ•°é‡ï¼Œæå‡æ³›åŒ–
    """
    
    def __init__(self, config: ModelConfig):
        super().__init__()
        self.config = config
        
        # è¯åµŒå…¥: 50257ä¸ªtokenï¼Œæ¯ä¸ªæ˜ å°„åˆ°768ç»´
        self.wte = nn.Embedding(config.vocab_size, config.n_embd)
        
        # ä½ç½®åµŒå…¥: 1024ä¸ªä½ç½®ï¼Œæ¯ä¸ªæ˜ å°„åˆ°768ç»´
        # å¯å­¦ä¹ çš„ä½ç½®ç¼–ç ï¼ˆåŒºåˆ«äºæ­£å¼¦ä½ç½®ç¼–ç ï¼‰
        self.wpe = nn.Embedding(config.block_size, config.n_embd)
        
        # Dropoutï¼ˆåµŒå…¥åï¼‰
        self.drop = nn.Dropout(config.dropout)
        
        # Transformerå—å †å ï¼ˆn_layerä¸ªï¼‰
        # nn.ModuleListç¡®ä¿PyTorchèƒ½è¯†åˆ«æ‰€æœ‰å­æ¨¡å—
        self.h = nn.ModuleList([Block(config) for _ in range(config.n_layer)])
        
        # æœ€ç»ˆLayerNorm
        self.ln_f = LayerNorm(config.n_embd, bias=config.bias)
        
        # è¯­è¨€æ¨¡å‹å¤´: 768ç»´ -> 50257ä¸ªtokençš„æ¦‚ç‡
        # bias=False: è¾“å‡ºå±‚ä¸ç”¨åç½®ï¼ˆè·ŸéšGPT-2è®¾è®¡ï¼‰
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        
        # ğŸ”‘ å…³é”®: æƒé‡ç»‘å®š
        # è¾“å…¥åµŒå…¥å’Œè¾“å‡ºæŠ•å½±å…±äº«åŒä¸€ä¸ªæƒé‡çŸ©é˜µ
        # å½¢çŠ¶éƒ½æ˜¯(vocab_size, n_embd)ï¼Œå¯ä»¥ç›´æ¥ç»‘å®š
        self.wte.weight = self.lm_head.weight
        
        # åˆå§‹åŒ–æ‰€æœ‰æƒé‡
        self.apply(self._init_weights)
        
        # ç‰¹æ®Šåˆå§‹åŒ–: æ®‹å·®æŠ•å½±çš„æƒé‡ç¼©å°
        # æ ¹æ®GPT-2è®ºæ–‡ï¼Œæ¯å±‚æ®‹å·®è·¯å¾„çš„æƒé‡åˆå§‹åŒ–æ ‡å‡†å·®è¦é™¤ä»¥sqrt(2*n_layer)
        # é˜²æ­¢æ·±å±‚ç½‘ç»œæ–¹å·®çˆ†ç‚¸
        for pn, p in self.named_parameters():
            if pn.endswith('c_proj.weight'):
                torch.nn.init.normal_(
                    p, 
                    mean=0.0, 
                    std=0.02 / math.sqrt(2 * config.n_layer)
                )
    
    def _init_weights(self, module):
        """
        æƒé‡åˆå§‹åŒ–ç­–ç•¥
        
        Linear: æ­£æ€åˆ†å¸ƒN(0, 0.02)
        Embedding: åŒæ ·N(0, 0.02)
        Bias: åˆå§‹åŒ–ä¸º0
        """
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
    
    def forward(
        self,
        idx: torch.Tensor,
        targets: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """
        å‰å‘ä¼ æ’­
        
        å‚æ•°:
            idx: è¾“å…¥token IDsï¼Œå½¢çŠ¶ (batch_size, seq_len)
            targets: ç›®æ ‡token IDsï¼ˆè®­ç»ƒæ—¶ç”¨ï¼‰ï¼Œç›¸åŒå½¢çŠ¶
        
        è¿”å›:
            logits: é¢„æµ‹åˆ†æ•° (batch_size, seq_len, vocab_size)
            loss: äº¤å‰ç†µæŸå¤±ï¼ˆå¦‚æœæä¾›äº†targetsï¼‰
        """
        device = idx.device
        b, t = idx.size()
        
        # æ£€æŸ¥åºåˆ—é•¿åº¦
        assert t <= self.config.block_size, (
            f"è¾“å…¥é•¿åº¦{t}è¶…è¿‡æœ€å¤§é•¿åº¦{self.config.block_size}"
        )
        
        # è¯åµŒå…¥: (b, t) -> (b, t, n_embd)
        tok_emb = self.wte(idx)
        
        # ä½ç½®åµŒå…¥: (t,) -> (t, n_embd)ï¼Œç„¶åå¹¿æ’­åˆ°(b, t, n_embd)
        pos_emb = self.wpe(torch.arange(t, device=device))
        
        # ç›¸åŠ : è¯­ä¹‰ä¿¡æ¯ + ä½ç½®ä¿¡æ¯
        x = self.drop(tok_emb + pos_emb)
        
        # é€šè¿‡æ‰€æœ‰Transformerå—
        for block in self.h:
            x = block(x)
        
        # æœ€ç»ˆå½’ä¸€åŒ–
        x = self.ln_f(x)
        
        # æŠ•å½±åˆ°è¯æ±‡è¡¨ç»´åº¦ï¼Œå¾—åˆ°æ¯ä¸ªä½ç½®çš„åˆ†æ•°
        logits = self.lm_head(x)  # (b, t, vocab_size)
        
        # è®¡ç®—æŸå¤±ï¼ˆå¦‚æœæä¾›äº†ç›®æ ‡ï¼‰
        loss = None
        if targets is not None:
            # å±•å¹³è®¡ç®—äº¤å‰ç†µ:
            # logits: (b*t, vocab_size)
            # targets: (b*t,)
            loss = F.cross_entropy(
                logits.view(-1, logits.size(-1)),
                targets.view(-1),
                ignore_index=-1  # å¿½ç•¥å¡«å……ä½ç½®ï¼ˆå¦‚æœæœ‰ï¼‰
            )
        
        return logits, loss
    
    def crop_block_size(self, block_size: int):
        """
        è£å‰ªæ¨¡å‹æ”¯æŒçš„æœ€å¤§åºåˆ—é•¿åº¦
        
        ç”¨é€”: å¾®è°ƒæ—¶å¦‚æœæ–°æ•°æ®è¾ƒçŸ­ï¼Œå¯ä»¥è£å‰ªä»¥èŠ‚çœè®¡ç®—
        """
        assert block_size <= self.config.block_size
        self.config.block_size = block_size
        
        # è£å‰ªä½ç½®åµŒå…¥
        self.wpe.weight = nn.Parameter(self.wpe.weight[:block_size])
        
        # æ›´æ–°å› æœæ©ç ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        for block in self.h:
            if hasattr(block.attn, 'bias'):
                block.attn.bias = block.attn.bias[:, :, :block_size, :block_size]
    
    @torch.no_grad()  # ç¦ç”¨æ¢¯åº¦ï¼ŒèŠ‚çœå†…å­˜
    def generate(
        self,
        idx: torch.Tensor,
        max_new_tokens: int,
        temperature: float = 1.0,
        top_k: Optional[int] = None
    ) -> torch.Tensor:
        """
        è‡ªå›å½’ç”Ÿæˆæ–‡æœ¬
        
        å‚æ•°:
            idx: åˆå§‹tokenåºåˆ— (batch_size, seq_len)
            max_new_tokens: è¦ç”Ÿæˆå¤šå°‘ä¸ªæ–°token
            temperature: é‡‡æ ·æ¸©åº¦ï¼ˆ<1æ›´ä¿å®ˆï¼Œ>1æ›´éšæœºï¼‰
            top_k: åªä»æ¦‚ç‡æœ€é«˜çš„kä¸ªtokenä¸­é‡‡æ ·ï¼ˆNone=å…¨éƒ¨ï¼‰
        
        è¿”å›:
            ç”Ÿæˆçš„å®Œæ•´åºåˆ— (batch_size, seq_len + max_new_tokens)
        """
        # é€ä¸ªtokenç”Ÿæˆ
        for _ in range(max_new_tokens):
            # å¦‚æœåºåˆ—å¤ªé•¿ï¼Œåªå–æœ€åblock_sizeä¸ª
            # è¿™æ˜¯æ¨¡å‹çš„æœ€å¤§è®°å¿†é•¿åº¦
            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]
            
            # å‰å‘ä¼ æ’­è·å–é¢„æµ‹
            logits, _ = self(idx_cond)
            
            # åªå–æœ€åä¸€ä¸ªä½ç½®çš„é¢„æµ‹ï¼ˆé¢„æµ‹ä¸‹ä¸€ä¸ªtokenï¼‰
            logits = logits[:, -1, :]  # (batch_size, vocab_size)
            
            # æ¸©åº¦ç¼©æ”¾: é™¤ä»¥temperature
            # T<1: åˆ†å¸ƒæ›´å°–é”ï¼Œé«˜æ¦‚ç‡tokenæ›´çªå‡º
            # T>1: åˆ†å¸ƒæ›´å¹³å¦ï¼Œå¢åŠ éšæœºæ€§
            logits = logits / temperature
            
            # Top-kç­›é€‰: åªä¿ç•™æ¦‚ç‡æœ€é«˜çš„kä¸ª
            if top_k is not None:
                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
                # ä½äºç¬¬kåçš„éƒ½è®¾ä¸º-infï¼ˆæ¦‚ç‡å˜ä¸º0ï¼‰
                logits[logits < v[:, [-1]]] = -float('Inf')
            
            # è®¡ç®—æ¦‚ç‡åˆ†å¸ƒ
            probs = F.softmax(logits, dim=-1)
            
            # ä»å¤šé¡¹åˆ†å¸ƒä¸­é‡‡æ ·ä¸€ä¸ªtoken
            idx_next = torch.multinomial(probs, num_samples=1)
            
            # æ‹¼æ¥åˆ°åºåˆ—æœ«å°¾
            idx = torch.cat((idx, idx_next), dim=1)
        
        return idx
    
    def get_num_params(self, non_embedding: bool = True) -> int:
        """
        è®¡ç®—æ¨¡å‹å‚æ•°é‡
        
        å‚æ•°:
            non_embedding: æ˜¯å¦æ’é™¤ä½ç½®åµŒå…¥ï¼ˆé€šå¸¸ä¸è¯åµŒå…¥å…±äº«ï¼‰
        
        è¿”å›:
            å‚æ•°æ€»æ•°
        """
        n_params = sum(p.numel() for p in self.parameters())
        if non_embedding:
            # å‡å»ä½ç½®åµŒå…¥ï¼ˆé€šå¸¸ä¸å…±äº«ï¼Œä¸ç®—å…³é”®å‚æ•°ï¼‰
            n_params -= self.wpe.weight.numel()
        return n_params
    
    def estimate_mfu(self, fwdbwd_per_iter: int, dt: float) -> float:
        """
        ä¼°è®¡æ¨¡å‹æµ®ç‚¹è¿ç®—åˆ©ç”¨ç‡ï¼ˆModel Flops Utilizationï¼‰
        
        ä¸A100ç­‰GPUçš„ç†è®ºå³°å€¼å¯¹æ¯”ï¼Œè¯„ä¼°è®­ç»ƒæ•ˆç‡
        
        å‚æ•°:
            fwdbwd_per_iter: æ¯æ¬¡è¿­ä»£çš„forward+backwardæ¬¡æ•°
            dt: æ¯æ¬¡è¿­ä»£çš„æ—¶é—´ï¼ˆç§’ï¼‰
        
        è¿”å›:
            MFUæ¯”ä¾‹ï¼ˆ0-1ä¹‹é—´ï¼Œè¶Šæ¥è¿‘1è¶Šå¥½ï¼‰
        """
        # ä¼°ç®—æ¯æ¬¡forward+backwardçš„æµ®ç‚¹è¿ç®—æ•°
        # åŸºäºPaLMè®ºæ–‡çš„å…¬å¼
        N = self.get_num_params()
        L, H, Q, T = (
            self.config.n_layer,
            self.config.n_head,
            self.config.head_dim,
            self.config.block_size
        )
        
        # æ¯ä¸ªtokençš„flops
        flops_per_token = 6 * N + 12 * L * H * Q * T
        # æ¯æ¬¡forward+backwardï¼ˆbackwardæ˜¯forwardçš„2å€ï¼‰
        flops_per_fwdbwd = flops_per_token * T
        # æ¯æ¬¡è¿­ä»£
        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter
        
        # å®é™…è¾¾åˆ°çš„flops
        flops_achieved = flops_per_iter * (1.0 / dt)
        
        # A100 bfloat16å³°å€¼çº¦312 TFLOPS
        flops_promised = 312e12
        
        mfu = flops_achieved / flops_promised
        return mfu


def create_model(config: ModelConfig) -> GPT:
    """
    å·¥å‚å‡½æ•°ï¼šæ ¹æ®é…ç½®åˆ›å»ºæ¨¡å‹
    
    ç”¨æ³•:
        from nanogpt_plus.config import ModelConfig
        from nanogpt_plus.models import create_model
        
        cfg = ModelConfig(n_layer=12, n_embd=768)
        model = create_model(cfg)
    """
    return GPT(config)